{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge of segmenting images with limited data was unique due to the scarcity of training data and the abundance of unlabeled data. Initially, I built my solution using a mix vision transformer, but thanks to discussions on the forum, I switched my approach. To tackle the challenge of limited data, I decided to create patches from the original images and use various pretrained MaxViTs from timm with different image sizes. However, with limited time to test my hypothesis, I chose to stick with the configurations that worked best for my most successful model and used an ensemble of various models with different training and validation sets.\n",
    "\n",
    "Given the limited data, I also chose to utilize an overfitting strategy, as the holdout and test sets were from the same distribution as the training set. Initially, I started with DiceLoss, but my MaxViT model was too strong for DiceLoss, so I decided to try Lovasz Loss, which worked well. To maintain the spatial and image ratio, I used padding, and for inference, I employed MONAI's sliding window inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image as Image\n",
    "from glob import glob\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from data_preparation import *\n",
    "from datasets import *\n",
    "from training import *\n",
    "from utils import *\n",
    "from model import CreateModel\n",
    "from pseudo_data import CreatePseudoSamples\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The provided code imports several model configurations from the model_config module and organizes them into a dictionary called model_configs_dict. This dictionary maps descriptive keys to their respective model configuration objects, facilitating easier access and management of multiple model configurations for training purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_config import (\n",
    "    maxxvit_rmlp_small_rw_256_exp_1, maxxvitv2_rmlp_base_rw_384_patch_256_exp_2, \n",
    "    maxxvitv2_rmlp_base_rw_224_exp_3, maxxvitv2_rmlp_base_rw_384_exp_4, \n",
    "    maxvit_rmlp_base_rw_224_scse_exp_5, maxvit_rmlp_base_rw_384_exp_6, \n",
    "    maxxvitv2_rmlp_base_rw_384_pseudo_exp_7, maxxvitv2_rmlp_base_rw_384_exp_8, \n",
    "    maxxvit_rmlp_small_rw_256_pseudo_exp_9, maxxvitv2_rmlp_base_rw_224_scse_exp_10, \n",
    "    maxxvit_rmlp_small_rw_256_patch_224_pseudo_exp_11\n",
    ")\n",
    "\n",
    "#definne all model configurations\n",
    "model_configs_dict = {\n",
    "    'maxxvit_exp_1': maxxvit_rmlp_small_rw_256_exp_1,\n",
    "    'maxxvitv2_exp_2': maxxvitv2_rmlp_base_rw_384_patch_256_exp_2,\n",
    "    'maxxvitv2_exp_3': maxxvitv2_rmlp_base_rw_224_exp_3,\n",
    "    'maxxvitv2_exp_4': maxxvitv2_rmlp_base_rw_384_exp_4,\n",
    "    'maxvit_exp_5': maxvit_rmlp_base_rw_224_scse_exp_5,\n",
    "    'maxvit_exp_6': maxvit_rmlp_base_rw_384_exp_6,\n",
    "    'maxxvitv2_exp_7': maxxvitv2_rmlp_base_rw_384_pseudo_exp_7,\n",
    "    'maxxvitv2_exp_8': maxxvitv2_rmlp_base_rw_384_exp_8,\n",
    "    'maxxvit_exp_9': maxxvit_rmlp_small_rw_256_pseudo_exp_9,\n",
    "    'maxxvitv2_exp_10': maxxvitv2_rmlp_base_rw_224_scse_exp_10,\n",
    "    'maxxvit_exp_11': maxxvit_rmlp_small_rw_256_patch_224_pseudo_exp_11,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Define all the directory paths here and load the configurations for the model you want to train from the above model configurations. \n",
    "##### NOTE - Some model require pseudo samples for training. Therefore, train them in specific order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and Configuration\n",
    "train_data_dir = 'stranger-sections-2-train-data'\n",
    "unlabeled_data_dir = 'stranger-sections-2-unlabeled-data/'\n",
    "patches_data_dir = 'patches'\n",
    "\n",
    "weights_dir = 'weights'\n",
    "test_root_dir = 'stranger-sections-2-test-data/stranger-sections-2-test-data/'\n",
    "pred_output_dir = 'model_predictions'\n",
    "\n",
    "# trained model weights directory\n",
    "model_checkpoints = 'model_checkpoints'\n",
    "\n",
    "# Choose model configuration\n",
    "config = model_configs_dict['maxxvit_exp_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed\n",
    "seed_everything(config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The next cell separates the data into train and validation sets. Depending on the model configuration, it generates pseudo labels for selected pseudo images from the unlabeled data provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training and validation images\n",
    "train_list, valid_list = prepare_data(config, train_dir=train_data_dir, unlabeled_dir=unlabeled_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This block of code generates patches from the original images and their corresponding labels after padding both. Padding is used to ensure that the patches extracted from the images and labels maintain their spatial alignment and integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train and validation patches\n",
    "train_patch_names, valid_patch_names = create_patches(config, train_dir=train_data_dir, output_dir=patches_data_dir, train_list=train_list, val_list=valid_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The code block creates datasets and dataloaders for training and validation using patches generated from the configured experiment and sets up dataloaders for batch processing during model training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Careful with defining the directory path to patches for trainimg and validation\n",
    "patch_root_dir = f'{patches_data_dir}/{config.exp_name}'\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = StrangerSectionsDataset(root_dir=patch_root_dir, filenames_list=train_patch_names, transforms=config.train_transform, mode='train')\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, num_workers=config.num_workers, shuffle=True)\n",
    "\n",
    "val_dataset = StrangerSectionsDataset(root_dir=patch_root_dir, filenames_list=valid_patch_names, transforms=config.test_transform, mode='val')\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, num_workers=config.num_workers, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Define model, loss, optimizer and schedulers here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 126_202_236\n"
     ]
    }
   ],
   "source": [
    "#create model\n",
    "model = CreateModel(config)\n",
    "\n",
    "#define losses, optimizer and schedulers\n",
    "criterion = smp.losses.LovaszLoss(mode='multiclass', per_image=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=config.min_lr, T_max=config.epochs * len(train_loader))\n",
    "scheduler1 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=config.patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete some variables to free up memory\n",
    "del train_list, valid_list, train_patch_names, valid_patch_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The following code block iterates through the training epochs, performing training and validation for each epoch.  The model's state is saved at the end of training, and the CUDA cache is cleared to manage memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation loop\n",
    "for epoch in range(1, config.epochs + 1):\n",
    "    start_time = time.time()\n",
    "    print(f'Starting epoch: {epoch}')\n",
    "    \n",
    "    # Training and validation\n",
    "    training_loss = train(model, train_loader, criterion, optimizer, config.device, scheduler)\n",
    "    val_results = evaluate(model, val_loader, criterion, config.device, scheduler1)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    valid_loss = val_results['loss'][0]\n",
    "    valid_jaccard = val_results['jaccard'][0]\n",
    "    \n",
    "    print(f'Epoch: {epoch}, Time: {elapsed_time:.2f}')\n",
    "    print(f'Train Loss: {training_loss:.4f}, Val Loss: {valid_loss:.4f}, Jaccard: {valid_jaccard:.4f}\\n')\n",
    "    \n",
    "# Save the model\n",
    "if not os.path.exists(weights_dir):\n",
    "    os.makedirs(weights_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(weights_dir, f\"unetplusplus_{config.exp_name}.pth\")\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model for config '{config.exp_name}' saved to {model_path}\")\n",
    "\n",
    "# Clear the cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test images list\n",
    "test_images_list = sorted(os.listdir(os.path.join(test_root_dir, 'image')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# create inference dataset and dataloader\n",
    "transform = A.Compose([ToTensorV2()])\n",
    "inference_dataset = Inference_dataset(test_root_dir, test_images_list, transform)\n",
    "inference_loader = DataLoader(inference_dataset, batch_size=config.test_batch_size, num_workers=config.num_workers, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and load trained weights\n",
    "model = CreateModel(config)\n",
    "\n",
    "model_path = os.path.join(weights_dir, f\"unetplusplus_{config.exp_name}.pth\")\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The code defines a sliding window inference process using MONAI's SlidingWindowInferer, configured with all the parameters. Predicted masks are generated, cleared to remove noise using clear_predictions, and saved as .npy files in the specified prediction output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sliding inferer from monai\n",
    "inferer = SlidingWindowInferer(roi_size=config.roi_size, sw_batch_size=config.sw_batch_size, overlap=config.overlap, mode=config.mode, padding_mode=config.padding_mode)\n",
    "\n",
    "model.eval()\n",
    "model.to(config.device)\n",
    "\n",
    "# generate predictions \n",
    "predictions = []\n",
    "filenames = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for image, file_path in tqdm(inference_loader):\n",
    "        image = image.to(config.device)\n",
    "        outputs = inferer(inputs=image, network=model)\n",
    "        \n",
    "        predicted_masks = torch.argmax(outputs, dim=1)\n",
    "        predictions.extend(predicted_masks.cpu().numpy())\n",
    "        filenames.extend(file_path)\n",
    "\n",
    "output_dir = os.path.join(pred_output_dir, f'{config.exp_name}')\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# clear images and save \n",
    "cleared_predictions = clear_predictions(predictions)\n",
    "for filename, pred in zip(filenames, cleared_predictions):\n",
    "    img_id = os.path.splitext(os.path.basename(filename))[0]\n",
    "    output_name = os.path.join(output_dir, f\"{img_id}_pred.npy\")\n",
    "    np.save(output_name, pred)\n",
    "\n",
    "print(f\"Prediction for {config.exp_name} saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NOTE - The provided code loops over all the model configurations and trains them in a single run. However, this approach might lead to out-of-memory issues. Therefore, it would be better to train each model configuration separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NOTE - Scroll down to make ensemble submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def train_and_evaluate(config, train_dir=None, unlabeled_dir=None, patches_dir=None):\n",
    "#    # Seed everything for reproducibility\n",
    "#    seed_everything(config.seed)\n",
    "#    \n",
    "#    train_list, valid_list = prepare_data(config, train_dir=train_dir, unlabeled_dir=unlabeled_dir)\n",
    "#\n",
    "#    # Create patches for training and validation\n",
    "#    train_patch_names, valid_patch_names = create_patches(config, train_dir=train_dir, output_dir=patches_dir, train_list=train_list, val_list=valid_list)\n",
    "#\n",
    "#    # Create datasets and dataloaders\n",
    "#    train_dataset = StrangerSectionsDataset(f'patches/{config.exp_name}', train_patch_names, transforms=config.train_transform, mode='train')\n",
    "#    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, num_workers=config.num_workers, shuffle=True)\n",
    "#\n",
    "#    val_dataset = StrangerSectionsDataset(f'patches/{config.exp_name}', valid_patch_names, transforms=config.test_transform, mode='val')\n",
    "#    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, num_workers=config.num_workers, shuffle=False)\n",
    "#    \n",
    "#    model = CreateModel(config)\n",
    "#    \n",
    "#    criterion = smp.losses.LovaszLoss(mode='multiclass', per_image=True)\n",
    "#    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "#    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=config.min_lr, T_max=config.epochs * len(train_loader))\n",
    "#    scheduler1 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=config.patience)\n",
    "#\n",
    "#    for epoch in range(1, config.epochs + 1):\n",
    "#        start_time = time.time()\n",
    "#        print(f'Starting epoch: {epoch}')\n",
    "#        \n",
    "#        # Training and validation\n",
    "#        training_loss = train(model, train_loader, criterion, optimizer, config.device, scheduler)\n",
    "#        val_results = evaluate(model, val_loader, criterion, config.device, scheduler1)\n",
    "#        \n",
    "#        elapsed_time = time.time() - start_time\n",
    "#        \n",
    "#        valid_loss = val_results['loss'][0]\n",
    "#        valid_jaccard = val_results['jaccard'][0]\n",
    "#        \n",
    "#        print(f'Epoch: {epoch}, Time: {elapsed_time:.2f}')\n",
    "#        print(f'Train Loss: {training_loss:.4f}, Val Loss: {valid_loss:.4f}, Jaccard: {valid_jaccard:.4f}\\n')\n",
    "#        \n",
    "#\n",
    "#    # Save the model\n",
    "#    weights_dir = \"weights\"\n",
    "#    if not os.path.exists(weights_dir):\n",
    "#        os.makedirs(weights_dir, exist_ok=True)\n",
    "#    \n",
    "#    model_path = os.path.join(weights_dir, f\"unetplusplus_{config.exp_name}.pth\")\n",
    "#    torch.save(model.state_dict(), model_path)\n",
    "#    \n",
    "#    # Explicitly delete the model and optimizer\n",
    "#    del model\n",
    "#    del optimizer\n",
    "#    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Paths and Configuration\n",
    "#train_data_dir = 'D:/CODE/competitions/StrangerSection2/stranger-sections-2-train-data/'\n",
    "#unlabeled_data_dir = 'D:/CODE/competitions/StrangerSection2/stranger-sections-2-unlabeled-data/'\n",
    "#patches_data_dir = 'patches'\n",
    "#\n",
    "#\n",
    "## training and evaluation\n",
    "#def main():\n",
    "#\n",
    "#    for exp_name, config in model_configs_dict.items():\n",
    "#        try:\n",
    "#            print(f\"Training model configuration: {exp_name}\\n\")\n",
    "#            train_and_evaluate(config, train_dir=train_data_dir, unlabelled_dir=unlabelled_data_dir, patches_dir=patches_data_dir)\n",
    "#        except Exception as e:\n",
    "#            print(f\"Error occurred while training {exp_name}: {e}\")\n",
    "#        finally:\n",
    "#            # Clearing CUDA cache\n",
    "#            torch.cuda.empty_cache()\n",
    "#\n",
    "#if __name__ == '__main__':\n",
    "#    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block will help generate predictions for the trained models in model-checkpoints directory. \n",
    "\n",
    "NOTE - Code to generate predictions for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Inference\n",
    "def predict_all_models(config, test_root_dir=None, weights_path=None, output_dir=None):\n",
    "\n",
    "    test_images_list = sorted(os.listdir(os.path.join(test_root_dir, 'image')))\n",
    "    \n",
    "    transform = A.Compose([ToTensorV2()])\n",
    "    inference_dataset = Inference_dataset(test_root_dir, test_images_list[:2], transform)\n",
    "    inference_loader = DataLoader(inference_dataset, batch_size=config.test_batch_size, num_workers=config.num_workers, shuffle=False)\n",
    "    \n",
    "    inferer = SlidingWindowInferer(roi_size=config.roi_size, sw_batch_size=config.sw_batch_size, overlap=config.overlap, mode=config.mode, padding_mode=config.padding_mode)\n",
    "\n",
    "    model = CreateModel(config)\n",
    "    \n",
    "    model_path = os.path.join(weights_path, f\"unetplusplus_{config.exp_name}.pth\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    model.to(config.device)\n",
    "\n",
    "    predictions = []\n",
    "    filenames = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, file_path in tqdm(inference_loader):\n",
    "            image = image.to(config.device)\n",
    "            outputs = inferer(inputs=image, network=model)\n",
    "            \n",
    "            predicted_masks = torch.argmax(outputs, dim=1)\n",
    "            predictions.extend(predicted_masks.cpu().numpy())\n",
    "            filenames.extend(file_path)\n",
    "\n",
    "    output_dir = os.path.join(output_dir, f'{config.exp_name}')\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    cleared_predictions = clear_predictions(predictions)\n",
    "    for filename, pred in zip(filenames, cleared_predictions):\n",
    "        img_id = os.path.splitext(os.path.basename(filename))[0]\n",
    "        output_name = os.path.join(output_dir, f\"{img_id}_pred.npy\")\n",
    "        np.save(output_name, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = 'weights'\n",
    "\n",
    "# trained model weights directory\n",
    "model_checkpoints = 'model_checkpoints'\n",
    "\n",
    "test_root_dir = 'stranger-sections-2-test-data/stranger-sections-2-test-data/'\n",
    "pred_output_dir = 'model_predictions'\n",
    "\n",
    "\n",
    "for exp_name, config in model_configs_dict.items():\n",
    "    try:\n",
    "        print(f\"Predicting with model configuration: {exp_name}\")\n",
    "        predict_all_models(config, test_root_dir, model_checkpoints, pred_output_dir)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while predicting with {exp_name}: {e}\")\n",
    "    finally:\n",
    "        # Clear CUDA cache to prevent out of memory errors\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Model Preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The code block loads all model predictions. The predictions are processes using an ensemble_predictions function, presumably combining them into a final prediction for each image by averaging the predictions from all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load directory path to model predictions\n",
    "all_model_preds_path = 'model_predictions'\n",
    "ensemble_output_folder = 'submission'\n",
    "\n",
    "preds_list = os.listdir(all_model_preds_path)\n",
    "\n",
    "def load_sub(pred_path):\n",
    "    label_list = os.listdir(pred_path)\n",
    "    sub_masks = []\n",
    "    filenames = []\n",
    "    for name in label_list:\n",
    "        fname = os.path.splitext(name)[0].split('_')[0]\n",
    "        mask_path = os.path.join(pred_path, f'{fname}_pred.npy')\n",
    "        mask = np.load(mask_path)\n",
    "        sub_masks.append(mask)\n",
    "        filenames.append(fname)\n",
    "    return sub_masks, filenames\n",
    "\n",
    "model_preds = []\n",
    "pred_filenames = None\n",
    "for pred_folder in preds_list:\n",
    "    pred_path = os.path.join(all_model_preds_path, pred_folder)\n",
    "    print(f\"Loading predictions from: {pred_path}\")\n",
    "    masks, filename = load_sub(pred_path)\n",
    "    model_preds.append(masks)\n",
    "    if pred_filenames is None:\n",
    "        pred_filenames = filename\n",
    "\n",
    "ensemble_preds = ensemble_predictions(model_preds)\n",
    "\n",
    "if not os.path.exists(ensemble_output_folder):\n",
    "    os.makedirs(ensemble_output_folder, exist_ok=True)\n",
    "\n",
    "for fname, pred in zip(pred_filenames, ensemble_preds):\n",
    "    output_name = os.path.join(ensemble_output_folder, f\"{fname}_pred.npy\")\n",
    "    np.save(output_name, pred)\n",
    "\n",
    "print(f\"\\nFinal ensemble saved to {ensemble_output_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
